# POLM

POLM(Preference Optimization for Language Model) provides means of alignment such like Direct Preference Optimization.
I leave some sample codes for Supervised Fine-Tuning(SFT), DPO, especially focus on Japanese dataset.

å—œå¥½ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å­¦ç¿’ã™ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚
ç‰¹ã«æ—¥æœ¬èªã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«æ³¨ç›®ã—ã¦ç›´æ¥å—œå¥½æœ€é©åŒ–(DPO)ã‚’é©ç”¨ã™ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã‚’å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚

## ğŸ“–æ–‡çŒ®

[Direct Preference Optimization, NeurIPS2024](https://neurips.cc/virtual/2023/oral/73865)

## ğŸ’»å‹•ä½œç’°å¢ƒ

|OS|Python|Cuda|
|:---|:---|:---|
|Ubuntu22.04|Python3.11|cuda12.1|

## âš™ï¸ç’°å¢ƒæ§‹ç¯‰

```bash
source setup.sh
export HUGGINGFACE_API_KEY=<Your API Key>
export WANDB_API_KEY=<Your API Key>
```

- `torch`ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®ã¿ã€è‡ªèº«ã®cudaã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨åˆã‚ã›ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã‚’ãŠã™ã™ã‚ã—ã¾ã™ã€‚
  - ä»¥ä¸‹ã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç”¨ã‚³ãƒãƒ³ãƒ‰ã‚’ç¢ºèª
  - [torchã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](https://pytorch.org/get-started/locally/)

> [!NOTE]
> é–‹ç™ºè€…ã¯å…¬é–‹æ™‚ç‚¹ã§`torch==2.3.0`ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

## ğŸ“‹TODO

- [x] ãƒãƒƒã‚«ã‚½ãƒ³ã§ã®ä½¿ç”¨ã‚³ãƒ¼ãƒ‰ã®å…¬é–‹
- [ ] ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’argparseã§æŒ‡å®šã™ã‚‹å½¢å¼ã«å¯¾å¿œ


## ğŸ”¥Training

- ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¢ãƒ‡ãƒ«ã¯ä»¥ä¸‹ã®1Bç´šã®LLMãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ä»¥ä¸‹ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚
  - ãƒ¢ãƒ‡ãƒ«: `llm-jp/llm-jp-1.3b-v1.0`
  - SFTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: `cl-nagoya/auto-wiki-qa`
  - DPOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: `ryota39/dpo-ja-45k`

### SFT

- ãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å ´åˆ

> [!NOTE]
> - GPUã®VRAMã¯16GBä»¥å†…ã«åã¾ã‚‹ã“ã¨ã‚’å‹•ä½œç¢ºèªæ¸ˆã¿ã§ã™ã€‚
>   - å¿…è¦ã«å¿œã˜ã¦ãƒ¢ãƒ‡ãƒ«ã®è¦æ¨¡ã‚„ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚

```python
python3 scripts/sft.py
```

- ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã‚’ä½¿ã£ãŸå­¦ç¿’(LoRA)ã®å ´åˆ

```python
python3 scripts/sft_lora.py
```

### SFTå¾Œã®ãƒ¢ãƒ‡ãƒ«ã‚’HuggingFaceã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹

> [!IMPORTANT]
> HuggingFaceã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã€HuggingFaceã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒç„¡ã„æ–¹ã¯å…ˆã«[HuggingFace](https://huggingface.co/)ã§ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ä½œæˆã—ã¾ã—ã‚‡ã†ã€‚

> [!TIP]
> ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦DPOã‚’è¡Œã†ã“ã¨ã‚‚å¯èƒ½ã§ã™ãŒã€ãƒ‘ã‚¹æŒ‡å®šã®ã‚¨ãƒ©ãƒ¼ãŒå¢—ãˆã‚‹ã¨åˆ¤æ–­ã—ãŸãŸã‚ã€ä¸€åº¦HuggingFaceã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸå¾Œã«ã€ãã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ–¹å¼ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚

- ãƒ•ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å ´åˆ
  - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯`private`ãƒ¢ãƒ¼ãƒ‰ã§ã®ãƒ¢ãƒ‡ãƒ«å…¬é–‹ã§ã™ã€‚

- base_model, tokenizer_model, new_modelã«ã‚³ãƒ¼ãƒ‰å†…ã®ä¾‹ã‚’å‚è€ƒã«ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚
```python
python3 scripts/push_to_hub.py
```

- ä½ãƒ©ãƒ³ã‚¯è¡Œåˆ—ã‚’ä½¿ã£ãŸå­¦ç¿’(LoRA)ã®å ´åˆ
  - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯`private`ãƒ¢ãƒ¼ãƒ‰ã§ã®ãƒ¢ãƒ‡ãƒ«å…¬é–‹ã§ã™ã€‚
- base_model, tokenizer_model, new_modelã«ã‚³ãƒ¼ãƒ‰å†…ã®ä¾‹ã‚’å‚è€ƒã«ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

```python
python3 scripts/push_to_hub_lora.py
```

### DPO


> [!CAUTION]
> - ç¾çŠ¶DPOã¯LoRAã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã€‚
> - `sft_lora.py`ã¨åŒæ§˜ã«ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’`target_modules`ã§æŒ‡å®šã—ã¦`peft_config`ã‚’`TrianingArgments`ã«æ¸¡ã›ã°DPOã‚‚LoRAã«å¯¾å¿œã§ãã¾ã™ã€‚


```python
python3 scripts/dpo.py
```

### DPOå¾Œã®ãƒ¢ãƒ‡ãƒ«ã‚’HuggingFaceã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹

```python
python3 scripts/push_to_hub.py
```

> [!NOTE]
> SFTã¨åŒæ§˜ã«ã€base_model, tokenizer_model, new_modelã«ã‚³ãƒ¼ãƒ‰å†…ã®ä¾‹ã‚’å‚è€ƒã«ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚

### ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–

```python 
python3 scripts/infer.py
```

> [!NOTE]
> ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯è‡ªåˆ†ãŒå­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ã€‚

> [!CAUTION]
> å…¬é–‹æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã¯ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã®å“è³ªãŒä½ã„ã“ã¨ã‚’ç¢ºèªã—ã¦ã„ã¾ã™ã€‚ã“ã®ã‚³ãƒ¼ãƒ‰ã‚’é€šã—ã¦å­¦ç¿’ã—ãŸãƒ¢ãƒ‡ãƒ«ã¯é–‹ç™ºä¸­ã§ã‚ã‚‹ã“ã¨ã‚’ã”ç•™æ„ãã ã•ã„ã€‚


## ğŸ“ãƒ¡ãƒ¢

- list of optimizer in Transformer library

```
ADAMW_HF = "adamw_hf"
ADAMW_TORCH = "adamw_torch"
ADAMW_TORCH_FUSED = "adamw_torch_fused"
ADAMW_TORCH_XLA = "adamw_torch_xla"
ADAMW_TORCH_NPU_FUSED = "adamw_torch_npu_fused"
ADAMW_APEX_FUSED = "adamw_apex_fused"
ADAFACTOR = "adafactor"
ADAMW_ANYPRECISION = "adamw_anyprecision"
SGD = "sgd"
ADAGRAD = "adagrad"
ADAMW_BNB = "adamw_bnb_8bit"
ADAMW_8BIT = "adamw_8bit"  # just an alias for adamw_bnb_8bit
LION_8BIT = "lion_8bit"
LION = "lion_32bit"
PAGED_ADAMW = "paged_adamw_32bit"
PAGED_ADAMW_8BIT = "paged_adamw_8bit"
PAGED_LION = "paged_lion_32bit"
PAGED_LION_8BIT = "paged_lion_8bit"
RMSPROP = "rmsprop"
RMSPROP_BNB = "rmsprop_bnb"
RMSPROP_8BIT = "rmsprop_bnb_8bit"
RMSPROP_32BIT = "rmsprop_bnb_32bit"
GALORE_ADAMW = "galore_adamw"
GALORE_ADAMW_8BIT = "galore_adamw_8bit"
GALORE_ADAFACTOR = "galore_adafactor"
GALORE_ADAMW_LAYERWISE = "galore_adamw_layerwise"
GALORE_ADAMW_8BIT_LAYERWISE = "galore_adamw_8bit_layerwise"
GALORE_ADAFACTOR_LAYERWISE = "galore_adafactor_layerwise"
```
